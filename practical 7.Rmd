---
title: "<span style='text-shadow: 1px, 1px, 4px rgba(34, 34, 34, .6); font-weight: 700; margin-top: 10px; margin-right: 60px; margin-bottom: 10px; margin-left: 60px'>Practical 7</span>"
author: "<span style='text-transform: uppercase; font-size: 1.5rem'>linear model</span>"
date: "Jodi Tay Seow Xuan"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    toc: true
    number_sections: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# The ![](diamond.jpg) data

In this exercise, predict the price of diamonds from a continuous and categorical variable. The data set is diamonds d of the package ggplot2, The data set is very large, so we will work with a random sample of 500 cases.

```{r}
library(ggplot2)
library(dplyr)
data("diamonds")
```

  a. Set the seed to 100 and—with the help of pipe operators—use the slice_sample() function to make a data set called my_diamonds, and change the class of the variable cut from ordered to factor.

```{r}
set.seed(100)
diamonds %>%
  slice_sample(n = 500) %>%
  mutate(cut     = factor(cut, ordered = FALSE),
         color   = factor(color, ordered = FALSE),
         clarity = factor(clarity, ordered = FALSE)) ->
  my_diamonds
```

---

## Numeric predictors

We start with a simple regression predicting the price of diamonds from the numeric predictors carat.

  a. Display a scatter plot of price against carat, and add a linear regression line (set the size of the points to 0.5).
  
```{r}
ggplot(my_diamonds, aes(carat, price)) +
  geom_point(size = 0.5) +
  geom_smooth(method = "lm", se = FALSE)
```
  
  b. Describe the effect of carat on price.

An increase in 1 carat increases the price of the diamond by $7926.  

  c. Display and interpret the summary of the model.

```{r}
lm_model <- lm(price ~ carat, my_diamonds)
summary(lm_model)
```

The intercept parameter suggests that diamonds that have a value of zero carats have a price of -$2320. This is not economically feasible as price cannot be negative. The carat coefficient suggests that for every 1 carat increase in a diamond there is a $7926 increase in its price, ceteris paribus.

  d. Test the significance of the model against the intercept-only model.

```{r}
anova(lm(price ~ 1, my_diamonds), lm_model)
```

  e. Display and interpret a the residual plots of the model, and interpret.

```{r}
par(mfrow = c(2, 2))
plot(lm_model)
```

The residuals vs fit plot suggests that the linearity assumption is not violated though there seem to be several outliers. However, the residuals are not normally distributed as a group of them stray away from the diagonal in the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is not violated, but the residuals are not randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals decrease along the axis.

## Categorical predictor

We now predicting the price of diamonds from the categorical predictors cut. The variables cut in this data set has class ordered (the class for ordinal variables), but, for this practical, we want it to have class factor (the class for categorical variables).

  a. Display boxplots of price for the different levels of cut. Interpret the result.

```{r}
ggplot(my_diamonds, aes(price, cut)) +
  geom_boxplot()
```

  b. Display and interpret the summary of the model with cut predicting price.

```{r}
lm_model_cut <- lm(price ~ cut, my_diamonds)
summary(lm_model_cut)
```

In this model, diamonds with a fair cut are the reference point. The coefficients for diamonds with a good and premium cut are not statistically significant at the 5% level as their p-values are too high. Compared to diamonds with a fair cut, diamonds with a very good cut are $2063.70 cheaper, ceteris paribus. Compared to diamonds with a fair cut, diamonds with an ideal cut are $2806.90 cheaper, ceteris paribus. 

  c. What is the reference category of cut?

```{asis}
Fair
```

  d. Which category has the highest predicted price?

```{asis}
Fair
```

  e. Display and interpret the residual plots of the model.

```{r}
par(mfrow = c(2, 2))
plot(lm_model_cut)
```

The residuals vs fit plot suggests that the linearity assumption is not violated. However, the residuals are not normally distributed as a group of them stray away from the diagonal in the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is not violated, but the residuals are not randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals decreases along the axis.

## Multiple regression

  a. Fit the main effects model with carat and cut as predictors of price, and display and interpret its summary

```{r}
lm_main_effects <- lm(price ~ carat + cut, my_diamonds)
summary(lm_main_effects)
```

An increase of 1 carat is associated with a $8053.30 increase in a diamond's price, ceteris paribus. Compared to diamonds with a fair cut, diamonds with a good cut, very good cut, premium cut, and ideal cut are priced higher by $1422, $1563.20, $1626.10, and $1813.80 respectively, ceteris paribus. 

  b. Qualitatively compare the size and significance of the parameter estimates from this model to those from the simple regression models.

Based on their p-values, all the coefficients of 'cut' from the main effects model are statistically significant at the 5% level. The coefficient for 'carat' is equally low for both models. This suggests that cut remains a significant predictor of price even when other variables are introduced.

The main effects model also has the highest adjusted R-squared of 0.8603, suggesting that it is the better fit of model to the data comapred to the simple regression models. The parameter estimate for carat is higher, implying that it has a stronger significance when considered alongside cut. The inverse is also true for cut.

  c. Fit a the same model with the interaction between carat and cut included, and display and interpret the model summary.

```{r}
lm_interaction <- lm(price ~ carat * cut, my_diamonds)
summary(lm_interaction)
```
The carat coefficient implies that for an increase of 1 carat, the diamond's price increases by $7529.40, ceteris paribus. All the coefficients for the different levels of cut are not statistically significant, suggesting that a diamond's price does not differ much between these levels, compared to fair cut diamonds. The interaction terms between carat and cut are not statistically significant at the 5% level, suggesting that the effect of carat on price does not differ significantly between the different levels of cut.

  d. What do you notice about the significance of the effects?

Except for the variable carat, the levels of cut and the interaction variables are all not statistically significant at the 5% level based on their p-values. This could be due to factors such as multicollinearity or heteroskedasticity, which can reduce the overall statistical significance.

e. Display a scatter plot with price on the x-axis, carat on the y-axis, with different colors for the points and linear regression lines for the levels of cut. Suppress the confidence bands. What does this plot tell you about the main effects and the interaction effect of carat and cut?

```{r}
ggplot(my_diamonds, aes(carat, price, col = cut)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```
The main effect of carat on price is positive across all levels of cut. The different cut qualities are vertically separated, with Ideal cut being the highest and Fair cut being the lowest price range. This implies that better cut grades tend to correspond to higher prices for a given carat. The effect of cut quality on price appears to be more pronounced at higher carats, with the lines diverging more as the number of carats increases. This implies that the higher price paid for better cut grades amplifies for diamonds with higher carats.

# Animals

In this exercise you will work with the Animals data from the package MASS. These data include two variables measuring the brain and body weight of various animals. The central question in this exercise is: “Can we predict the weight of an animal’s brain from the weight of the animal’s body? The exercise shows that with some simple data transformations and adding a predictor we can go from 0% variance explained to almost 100%!

## Linear model

a. Check the help file for Animals, and display the row names to get an idea of what kinds of animals are in the data.

```{r}
library(MASS)
data("Animals")
row.names(Animals)
```

  b. Let’s start with with some data visualizations. Display the boxplots showing the distributions of brain and body.

```{r}
library(gridExtra)
brain_box <- ggplot(Animals, aes(y = brain)) +
  geom_boxplot()

body_box <- ggplot(Animals, aes(y = body)) +
  geom_boxplot()

grid.arrange(brain_box, body_box, ncol = 2)
```

  c. Display a scatter plot with body on the x-axis and brain on the y-axis, and add a linear regression line including the confidence band. Interpret the plot.

```{r}
ggplot(Animals, aes(body, brain)) +
  geom_point() +
  geom_smooth(method = "lm")
```

The negative slope of the regression line suggests that as an animal's body weight increases, their brain weight tends to decrease. The widening of the confidence band at higher body weights indicates greater variability and uncertainty in the brain-body relationship for heavier animals.

  d. Let’s see how the linear regression model performs. Fit the linear model m0 predicting brain from body; display its summary, and interpret the parameter estimates and the R-squared.

```{r}
lm_m0 <- lm(brain ~ body, Animals)
summary(lm_m0)
```
The coefficient of body is not statistically significant at the 5% level, so we cannot draw conclusions about the effect of body mass on brain mass. The R^2 suggests that only 0.002853% of the variation in brain mass can be explained by body mass. This extremely low R^2 suggests that the model has a poor fit to the data.

  e. Display the diagnostic residual plots for the previous model, and interpret the results.

```{r}
par(mfcol = c(2, 2))
plot(lm_m0)
```

The residuals vs fit plot suggests that the linearity assumption is violated though there seem to be two outliers. The residuals are normally distributed according to the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is violated and the residuals are not randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals increases along the axis.

## Data transformations

Since the variables brain and body are substantially skewed, a data transformation may improve the fit of the model.

  a. Apply transformations to normalize the distributions of the variables body and brain, and check the result with boxplots.

```{r}
body_log <- ggplot(data = Animals, aes(y = log(body))) +
  geom_boxplot()

brain_log <- ggplot(data = Animals, aes(y = log(brain))) +
  geom_boxplot()

grid.arrange(body_log, brain_log, ncol = 2)
```

  b. Display a scatter plot of the transformed versions of brain and body, and add a linear regression line.

```{r}
ggplot(data = Animals, aes(log(body), log(brain))) +
  geom_point() +
  geom_smooth(method = "lm")
```

  c. Fit the linear model m1 with the transformed body predicting the transformed brain; display its summary, and interpret the parameter estimates and the R-squared.

```{r}
m1 <- lm(log(brain) ~ log(body), Animals)
summary(m1)
```
For a 1% increase in body mass, an animal's brain mass increases by 49.60%, ceteris paribus. The R^2 value indicates that approximately 60.76% of the variation in brain mass can be explained by body mass. This value is rather high, suggesting a reasonably good fit of the model to the data.

  d. Display the diagnostic plots for m1 and interpret. Are the model assumptions fulfilled?

```{r}
par(mfcol = c(2, 2))
plot(m1)
```

The residuals vs fit plot suggests that there is non-linearity and that there seem to be several outliers. The residuals are normally distributed according to the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is violate though the residuals are not randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals decreases along the axis.

## Adding a predictor

Additional predictors can help to satisfy the assumptions. For the data at hand, the residuals plots show three outliers that have one thing at common; they are prehistoric species. Let’s see what happens if we add a predictor to the model that distinguishes between extinct and other species.

  a. Make the factor species with levels “extinct” for the prehistoric animals and “other” for the others, and add this factor to the data frame Animals.

```{r}
Animals <- mutate(Animals, 
                  species = recode_factor(rownames(Animals),
                                          Brachiosaurus = "extinct",
                                          Triceratops   = "extinct",
                                          Dipliodocus   = "extinct", 
                                          .default      = "other")
                  )
```

  b. Fit the model m2 by adding the predictor species to model m1, display its summary, and interpret the parameter estimates and the R-squared.

```{r}
m2 <- update(m1, . ~ . + species, Animals)
summary(m2)
```
For a 1% increase in body mass, an animal's brain mass increases by 74.86%, ceteris paribus. Animals categorised as 'others' under the species variable have 5.22% higher brain mass than animals categorised as 'extinct', ceteris paribus. The adjusted R^2 indicates that approximately 91.31% of the variation in brain mass can be explained by body mass and species type. This value is very high, suggesting a good fit of the model to the data.

  c. Display and interpret the diagnostic plots for m2.

```{r}
par(mfcol = c(2, 2))
plot(m2)
```

The residuals vs fit plot suggests that the linearity assumption is not violated. The residuals are normally distributed according to the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is not violated and that the residuals are randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals decreases along the axis.

The diagnostic plots show outliers that have a higher (logarithm of) brain than predicted by the model. These outliers belong to the class of species know as primates. This result suggests that the predictor species could be extended with the category primates.

  d. Add the level “primate” to the variable species (check which of the animals are primates).

```{r}
Animals  <- mutate(Animals, species = recode_factor(rownames(Animals),
                                  'Brachiosaurus' = "extinct",
                                  'Triceratops'   = "extinct",
                                  'Dipliodocus'   = "extinct", 
                                  'Human'         = "primate",
                                  'Chimpanzee'    = "primate",
                                  'Gorilla'       = "primate",
                                  'Rhesus monkey' = "primate",
                                  'Potar monkey'  = "primate",
                                  .default        = "other")
                   )
```

  e. Fit the model m3 with the modified predictor species; display its summary, and interpret the parameter estimates and the R-squared.

```{r}
m3 <- update(m1, . ~ . + species, Animals)
summary(m3)
```
For a 1% increase in body mass, an animal's brain mass increases by 73.20%, ceteris paribus. Animals categorised as 'primate' under the species variable have 6.20% higher brain mass than animals categorised as 'extinct', ceteris paribus. Animals categorised as 'others' under the species variable have 4.90% higher brain mass than animals categorised as 'extinct', ceteris paribus. The adjusted R^2 indicates that approximately 95.06% of the variation in brain mass can be explained by body mass and species type. This value is very high, suggesting a good fit of the model to the data.

  f. Display and interpret the diagnostic plots for m3. What do the residual plots say about the brain weight of the Gorilla?

```{r}
par(mfcol = c(2, 2))
plot(m3)
```

The residuals vs fit plot suggests that the linearity assumption is not violated. The residuals are normally distributed according to the QQ plot. The scale-location plot implies that the assumption of homoscedasticity is not violated and that the residuals are randomly scattered around the red line. Lastly, the residuals vs leverage plot indicates heteroskedasticity as the spread of standardised residuals decreases along the axis.

  g. Test the fit of the models m1 to m3 with the anova() function.

```{r}
anova(m1, m2, m3)
```

---

End of practical